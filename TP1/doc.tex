\documentclass[a4paper,francais]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}

\usepackage{subfig}
\usepackage{graphicx}
\graphicspath{{fig/}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{cancel}

\usepackage{hyperref}

\usepackage{cprotect} %verbatim in footnote

\newcommand{\cad}{c.-à-d.}
\newcommand{\Z}{{\ensuremath\mathbb{Z}}}
\newcommand{\N}{{\ensuremath\mathbb{N}}}
\newcommand{\R}{{\ensuremath\mathbb{R}}}
\newtheorem{Theorem}{Theorem}

%-------- enable or disable correction -----------------------------
\theoremstyle{definition}
\newtheorem{exercice}{Exercice}[section]
\newtheorem*{solution}{Solution}

\usepackage{comment}
\excludecomment{solution}% commenter/décommenter pour afficher/effacer l'impression des solutions


\title{Optimisation sous contraintes linéaires}
\author{Tristan Roussillon}
\date{7 mars 2022}
  
\begin{document}

\maketitle

Ce TP aborde le problème de l'optimisation linéaire, défini,
pour des fonctions $f$ et $\{g_i\}_{\{1, \dots, m\}}$ linéaires
\footnote{le terme exact serait \emph{affine}, mais \emph{linéaire}
  est le terme consacré pour mimer l'anglais, parce que le graphe
  d'une fonction affine est une ligne droite et parce qu'il
  existe une transformation simple permettant de passer d'une
  fonction affine en une fonction linéaire.}
comme :
\[
\text{(PL)} \left\{
\begin{array}{c}
  \min_{x} \ f(x) \ \text{tel que :} \\
  g_i(x) \leq 0, \ i \in I = \{1, \dots, m\} \\
  x \in  \R^n \\
\end{array}
\right.
\]

(PL) peut
\begin{itemize}
\item être \emph{infaisable} s'il n'y aucune solution,
  c'est-à-dire aucun $x$ ne satisfaisant les contraintes,
\item \emph{ne pas être borné} si, d'une solution donnée, on peut
  toujours se déplacer vers une solution qui fait décroitre $f$,
\item avoir une ou plusieurs solutions optimales, situées
  sur le bord du polytope convexe décrit par les contraintes,
\item être résolu par la méthode du \emph{simplexe}
  (en temps exponentiel au pire cas, mais souvent linéaire en
  pratique) ou celle \emph{des points intérieurs} (en temps
  polynomial et dont les implémentations récentes sont encore
  plus performantes). Il existe une multitude de solveurs,
  mais nous utiliserons la bibliothèque python
  \href{https://docs.scipy.org/doc/}{\texttt{SciPy}} dans
  ce TD.
\end{itemize} 

\section{Existence et multiplicité des solutions}
\label{sec:sol}

\begin{exercice}
  \label{ex:ex}
  Considérons les 4 propositions suivantes :
  \begin{itemize}
  \item[(a)] Le problème est infaisable,
  \item[(b)] Le problème est non borné,
  \end{itemize}
  et, en notant $X$ l'ensemble des
  solutions candidates vérifiant les contraintes,
  \begin{itemize}
  \item[(c)] La solution optimale se trouve sur un sommet du bord de $X$,
  \item[(d)] Les solutions optimales se trouvent sur une arête du bord de $X$.
  \end{itemize}
  
  Considérons également les 4 problèmes suivants :
  %(b) non borné
  \[
  \text{(P1)}
  \left\{
  \begin{array}{c}
    \min_{x_1,x_2} \ -x_1 - x_2 - 1 \ \text{tel que :} \\
    \begin{array}{ll}
      -x_1 \leq 0 \\
      -x_2 \leq 0 \\
    \end{array} 
  \end{array}
  \right.
  \]
  %(a) infaisable
  \[
  \text{(P2)}
  \left\{
  \begin{array}{c}
    \min_{x_1,x_2} \ x_1 \ \text{tel que :} \\
    \begin{array}{ll}
      -x_1 \leq 0 \\
      -x_2 \leq 0 \\
      x_1 + x_2 + 1 \leq 0 \\
    \end{array} 
  \end{array}
  \right.
  \]
  %arete
  \[
  \text{(P3)}
  \left\{
  \begin{array}{c}
    \min_{x_1,x_2} \ x_1 \ \text{tel que :} \\
    \begin{array}{ll}
      -x_1 \leq 0 \\
      -x_2 \leq 0 \\
      x_1 + x_2 - 1 \leq 0 \\
    \end{array} 
  \end{array}
  \right.
  \]
  %sommet
  \[
  \text{(P4)}
  \left\{
  \begin{array}{c}
    \min_{x_1,x_2} \ -x_1+x_2 \ \text{tel que :} \\
    \begin{array}{ll}
      -x_1 \leq 0 \\
      -x_2 \leq 0 \\
      x_1 + x_2 - 1 \leq 0 \\
    \end{array} 
  \end{array}
  \right.
  \]

  Donnez la correspondance entre les 4 propositions et les 4 problèmes
  à l'aide de représentations graphiques.
\end{exercice}

\begin{exercice}
  Nous considérons maintenant plus généralement le problème (PL) défini
  en introduction. 
  \begin{enumerate}
  \item Est-ce que l'hypothèse de qualification des contraintes est vérifiée
    pour tout $x$ ? Quelle partie du cours permet de répondre à cette question ?
  \item On suppose maintenant que l'ensemble des solutions candidates vérifiant
    les contraintes, noté $X$, a un intérieur non vide. Montrez par contradiction et en utilisant les conditions de Kuhn et Tucker qu'une solution optimale de (PL) ne peut appartenir à l'intérieur de $X$.
    %% qu'il n'existe
    %% aucun $x \in X \setminus \partial X$\footnote{$\partial X$ désigne le bord
    %%   de $X$ tandis que $X \setminus \partial X$ désigne son intérieur.}
    %% tel que les conditions de Kuhn et Tucker soient vérifiées.
  \end{enumerate}
\end{exercice}

\begin{solution}
  \begin{enumerate}
  \item Oui car la fonction et les contraintes sont linéaires.
  \item Si $x \in X \setminus \partial X$, par définition aucune contrainte n'est saturée,
  ce qui implique que tous les coefficients $\lambda_i$ sont nuls. La condition
  devient ${\nabla f}(x) = 0$, ce qui est faux pour des fonctions linéaires (et non
  constantes).
  \end{enumerate}
\end{solution}

\section{Classification linéaire}
\label{sec:classif}

\let\vec\mathbf

On suppose donnés des points de $\R^2$, $\{\vec{u}^j \ | \ {j \in J} \}$ partitionnés en deux sous-ensembles :
un sous-ensemble d'observations positives $S^+ := \{ \vec{u}^j, j \in J^+ \}$ et un sous-ensemble
d'observations négatives $S^- := \{ \vec{u}^j, j \in J^- \}$
(avec $J = J^+ \cup J^-$ et $J^+ \cap J^- = \emptyset$).
On veut déterminer une règle de décision linéaire permettant de discriminer au mieux
les observations positives et les observations négatives.

\begin{exercice}
  Considérons le problème suivant : 
  \[
  \text{(P0)}
  \left\{
  \begin{array}{ll}
    \max_{\vec{x},z^+,z^-} \ (z^+ - z^-) \ \text{tel que} & \\
    \vec{x} \cdot \vec{u}^j \geq z^+ & \forall j \in J^+ \\ 
    \vec{x} \cdot \vec{u}^j \leq z^- & \forall j \in J^- \\
    \vec{x} \in \R^2, z^+, z^- \in \R \\
  \end{array}
  \right.
  \]

  \begin{enumerate}
  \item Vérifiez que (P0) est un problème d'optimisation linéaire.
  \item A partir de valeurs de $\vec{x}, z^+, z^-$ vérifiant les
    contraintes, donnez l'équation d'une droite séparant $S^+$ et $S^-$. 
  \item Expliquez pourquoi (P0) n'est pas borné dès qu'il existe
    une solution candidate telle que $z^+ \neq z^-$.
  \end{enumerate}

\end{exercice}

\begin{solution}
  Item 3: 
  S'il existe des valeurs pour $\vec{x}, z^+, z^-$ telles que
  les contraintes sont vérifiées et, sans perte de généralité,
  $(z^+ - z^-) > 0$, alors on peut les multiplier par une
  constante $\lambda > 1$ pour avoir une valeur $\lambda(z^+ - z^-)$ plus
  grande tout en vérifiant encore les contraintes. 
\end{solution}

Nous allons maintenant nous intéresser à deux formulations
légèrement différentes permettant d'avoir des problèmes
d'optimisation linéaire bornés.

\begin{exercice}
  Considérons le problème suivant : 
  \[
  \text{(P$\infty$)}
  \left\{
  \begin{array}{ll}
    \max_{\vec{x},z^+,z^-} \ (z^+ - z^-) \ \text{tel que} & \\
    \vec{x} \cdot \vec{u}^j \geq z^+ & \forall j \in J^+ \\ 
    \vec{x} \cdot \vec{u}^j \leq z^- & \forall j \in J^- \\
    -1 \leq x_k \leq 1 & \forall k \in \{1, 2\} \\
    \vec{x} \in \R^2, z^+, z^- \in \R \\
  \end{array}
  \right.
  \]

  Expliquez pourquoi (P$\infty$) est borné. 
\end{exercice}

\begin{solution}
  S'il existe des valeurs pour $\vec{x}, z^+, z^-$ telles que
  les contraintes sont vérifiées et, sans perte de généralité,
  $(z^+ - z^-) \geq 0$, alors on peut les multiplier par une
  constante $\lambda \geq 1$ de façon à ce que $|\lambda x_1|=1$
  ou $|\lambda x_2|=1$
  (la norme $L_\infty$ de $\lambda\vec{x}$ vaut alors $1$).
  En faisant cela, les autres contraintes restent vérifiées
  et on a une valeur $\lambda(z^+ - z^-) \geq (z^+ - z^-)$.
\end{solution}


\begin{exercice}
  Réexprimez le problème (P$\infty$)
  sous la forme matricielle suivante :
  \[
  \left\{
  \begin{array}{c}
    \min_x \ c^T x \ \text{tel que :} \\
    Ax \leq b \\
  \end{array}
  \right. 
  \]
  où 
\begin{itemize}
\item $x$ est un vecteur réprésentant une solution. S'il y a $n$ inconnues, c'est un vecteur $n \times 1$.
\item $c$ est un vecteur de même dimension que $x$ et sert à définir la fonction objectif.
\item $A$ et $b$ servent à décrire $m$ inégalités linéaires : $b$ est un vecteur $m \times 1$,
  $A$ est une matrice $m \times n$ et le signe $\leq$ s'applique composante par composante et pour
  toutes les composantes des vecteurs.
\end{itemize}
%% Enfin, ignorez les contraintes impliquant des bornes inférieures ou supérieures sur les variables,
%% car elles sont souvent traitées de manière séparée par les solveurs. 
\end{exercice}

\begin{exercice}
  En reprenant le code Python disponible sur Moodle\footnote{
    le code utilise la version 1.3.3 de SciPy qui est la version
    disponible dans les dépôts sur Ubuntu 20.04.},
  programmez la résolution du problème (P$\infty$). %% et (P1).
  Que vous utilisiez 
  \href{https://docs.scipy.org/doc/scipy-1.3.3/reference/generated/scipy.optimize.linprog.html#scipy.optimize.linprog}{\texttt{linprog}} ou
  \href{https://docs.scipy.org/doc/scipy-1.3.3/reference/generated/scipy.optimize.minimize.html}{\texttt{minimize}},
  le plus délicat est de décrire correctement les données nécessaires au solveur.
  Vous aurez aussi besoin de
  \href{https://docs.scipy.org/doc/numpy/reference/routines.array-creation.html}{créer} et
  \href{https://docs.scipy.org/doc/numpy/reference/routines.array-manipulation.html}{manipuler} des
  tableaux
  \href{https://numpy.org/doc/stable/contents.html#numpy-docs-mainpage}{\texttt{NumPy}}. 
\end{exercice}

\begin{solution}
  Il faut arriver à construire correctement les vecteurs et les matrices nécessaires
  au solveur.
  %% , comme sur la Fig.~\ref{fig:stdForm} pour le cas~\ref{L1}.
  Le code complet se trouve sur Moodle.  
  %% \begin{figure}[htbp]
  %% \centering
  %% \includegraphics[width=0.49\textwidth,page=1]{standardForm}
  %% \includegraphics[width=0.49\textwidth,page=2]{standardForm}
  %% \caption{}
  %% \label{fig:stdForm}
  %% \end{figure}
\end{solution}

\begin{exercice}
  Nous réexprimons maintenant chaque variable $x_k$
  comme la différence de deux variables non-négatives,
  notées $x_k'$ et $x_k''$. 

  Codez aussi la résolution du problème suivant: 
  \[
  \text{(P1)}
  \left\{
  \begin{array}{ll}
    \max_{x_1',x_1'',x_2',x_2'',z^+,z^-} \  (z^+ - z^-) \ \text{tel que} & \\
    \sum_k (x_k' - x_k'') u_k^j \geq z^+ & \forall j \in J^+ \\ 
    \sum_k (x_k' - x_k'') u_k^j \leq z^- & \forall j \in J^- \\
    \sum_k (x_k' + x_k'') \leq 1 \\
    x_k' \geq 0, x_k'' \geq 0 & \forall k \in \{1, 2\} \\
    x_1',x_1'',x_2',x_2'',z^+,z^- \in \R \\
  \end{array}
  \right.
  \]
\end{exercice}

\begin{solution}
  S'il existe des valeurs telles que
  les contraintes sont vérifiées et, sans perte de généralité,
  $(z^+ - z^-) \geq 0$, alors on peut les multiplier par une
  constante $\lambda \geq 1$ de façon à ce que $\sum_k (x_k' + x_k'') = 1$.
  En faisant cela, les autres contraintes restent vérifiées
  et on a une valeur $\lambda(z^+ - z^-) \geq (z^+ - z^-)$.
  
  Pour conclure, il suffit alors de vérifier que :
  \[ \sum_k |x_k| \leq \sum_k (x_k' + x_k''). \]
  En effet, comme $x_k'$ et $x_k''$ sont non-négatives, $\forall k \in \{1, 2\}$,
  si $x_k \leq 0$, alors $x_k \leq 0 \leq x_k' + x_k''$
  et si $x_k > 0$, alors $x_k \leq x_k + 2x_k'' \Leftrightarrow x_k' - x_k'' \leq x_k' + x_k''$,
  d'où $|x_k| \leq x_k' + x_k''$.
\end{solution}

%% \section{Pour aller plus loin : séparateurs à vaste marge}
%% \label{sec:svm}

%% Les problèmes précédents sont très proches de celui considéré dans les
%% méthodes d'apprentissage connues sous le nom de \emph{Support Vector Machines}
%% \href{https://en.wikipedia.org/wiki/Support-vector_machine}{(SVM)} ou
%% \emph{Séparateurs à Vaste Marge} en français. Ce sont des classifieurs
%% linéaires\footnote{Ils peuvent aussi être adaptés au cas où les données
%%   ne sont pas parfaitement séparables ou au cas où les données sont
%%   non-linéairement séparables.} 
%% qui ont de meilleurs capacités de généralisation que le perceptron 
%% parce qu'ils maximisent la marge de séparation.

%% Il s'agit d'un problème d'optimisation quadratique sous contraintes
%% linéaires : 
%% \[
%% \left\{
%% \begin{array}{ll}
%%   \min_{\vec{x},z} \|\vec{x}\|_2^2 \ \text{tel que} & \\
%%   \vec{x} \cdot \vec{u}^j \geq z + 1 & \forall j \in J^+ \\ 
%%   \vec{x} \cdot \vec{u}^j \leq z - 1 & \forall j \in J^- \\
%%   \vec{x} \in \R^2
%% \end{array}
%% \right.
%% \]

%% \begin{exercice}
%%   Complétez le code de la section précédente pour ajouter la résolution du problème
%%   ci-dessus par l'algorithme de
%%   \href{https://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm}{Franck et Wolfe}. 
%% \end{exercice}

\end{document}


